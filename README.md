 ğŸ¤– Robotic Arm Pick and Place Simulation

A complete simulation of a **robotic arm** trained using **Reinforcement Learning (PPO)** in **PyBullet**, capable of performing autonomous **pick-and-place tasks** â€” with visual explanation using **Grad-CAM**.

---
 ğŸ§  Key Features

- ğŸ”§ Simulated realistic robotic arm environment using PyBullet
- ğŸ¤– Trained using **Proximal Policy Optimization (PPO)** from Stable Baselines3
- ğŸ¯ Pick-and-place task with **randomized object locations** for better generalization
- ğŸ“ Learned grasping through **reward shaping**
- ğŸ”¥ Visualized decision-making using **Grad-CAM heatmaps** over input images

---
ğŸ“ Project Structure

```bash
robot_arm/
â”œâ”€â”€ env/
â”‚   â””â”€â”€ robot_env.py           # Custom Gym environment
â”œâ”€â”€ train.py                   # Training script using PPO
â”œâ”€â”€ evaluate.py                # Evaluates trained model
â”œâ”€â”€ evaluate_with_gradcam.py   # Grad-CAM explanation
â”œâ”€â”€ robot_arm_model.zip        # Trained model
â”œâ”€â”€ gradcam_step_*.png         # Heatmap images
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

ğŸš€ Getting Started :

1ï¸âƒ£ Clone this repo:
git clone https://github.com/umme-aiman20/robot-arm-pick-and-place.git
cd robot-arm-pick-and-place

2ï¸âƒ£ Install dependencies:
pip install -r requirements.txt

3ï¸âƒ£ Run simulation with Grad-CAM:
python evaluate_with_gradcam.py


ğŸ§  How Grad-CAM Works Here
Grad-CAM highlights image regions that the RL policy focuses on when choosing actions.
Youâ€™ll see colorful heatmaps overlayed on camera views, showing the modelâ€™s visual attention.

ğŸ“œ License
This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.


ğŸ™‹â€â™€ï¸ Author:  
. ğŸ‘©ğŸ»â€ğŸ’» @umme-aiman20




